{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: The Evolution to Production\n",
    "\n",
    "## CML Models (Development) â†’ AI Inference Service (Production)\n",
    "\n",
    "### The Complete MLOps Journey\n",
    "\n",
    "- **Module 1**: Deployed sklearn model to CML Models for development/testing\n",
    "- **Module 2**: Detected data drift signaling model degradation  \n",
    "- **Module 3**: Retrained model and converted to ONNX format\n",
    "- **Module 4**: Deployed ONNX model to production AI Inference Service â† **We are here**\n",
    "\n",
    "### What We'll Compare\n",
    "\n",
    "1. **Authentication**: API Key vs JWT Token\n",
    "2. **API Protocol**: Custom REST vs Open Inference Standard\n",
    "3. **Performance**: Latency and throughput measurements\n",
    "4. **Operations**: Monitoring and enterprise capabilities\n",
    "\n",
    "This hands-on comparison gives you the \"before/after\" story to tell customers about production ML deployment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# For AI Inference Service\n",
    "import httpx\n",
    "from open_inference.openapi.client import OpenInferenceClient, InferenceRequest\n",
    "\n",
    "# Add module1 to path for preprocessing utilities\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "# Add module1 for helpers  \n",
    "sys.path.insert(0, os.path.abspath('../module1'))\n",
    "\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")\n",
    "print(f\"   Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Test Data\n",
    "\n",
    "We need two different data formats:\n",
    "- **Module 1 (CML)**: Expects fully preprocessed data (64 features after one-hot encoding + feature engineering)\n",
    "- **Module 4 (ONNX)**: Expects original 20 features with underscores (no engineering, no encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import PreprocessingPipeline, FeatureEngineer\n",
    "\n",
    "# Load raw inference data\n",
    "df_raw = pd.read_csv(\"../module1/inference_data/raw_inference_data.csv\", sep=\";\")\n",
    "print(f\"âœ… Loaded raw data: {df_raw.shape}\")\n",
    "\n",
    "# Load training data for preprocessing fit\n",
    "df_train = pd.read_csv(\"../module1/data/bank-additional/bank-additional-full.csv\", sep=\";\")\n",
    "print(f\"âœ… Loaded training data: {df_train.shape}\")\n",
    "\n",
    "# Apply feature engineering\n",
    "fe = FeatureEngineer()\n",
    "df_train_eng = fe.transform(df_train)\n",
    "df_engineered = fe.transform(df_raw)\n",
    "\n",
    "print(f\"âœ… Feature engineering applied\")\n",
    "print(f\"   Engineered features: {df_engineered.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features for preprocessing\n",
    "numeric_features = [\n",
    "    'age', 'duration', 'campaign', 'pdays', 'previous',\n",
    "    'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed',\n",
    "    'engagement_score'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'job', 'marital', 'education', 'default',\n",
    "    'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome',\n",
    "    'age_group', 'emp_var_category', 'duration_category'\n",
    "]\n",
    "\n",
    "# Create and fit preprocessor on training data (for CML endpoint)\n",
    "preprocessor = PreprocessingPipeline(\n",
    "    numeric_features=numeric_features,\n",
    "    categorical_features=categorical_features,\n",
    "    include_engagement=True\n",
    ")\n",
    "\n",
    "X_train_full = df_train_eng[numeric_features + categorical_features].copy()\n",
    "preprocessor.fit(X_train_full)\n",
    "\n",
    "# Transform inference data for CML endpoint (fully preprocessed)\n",
    "X_engineered = df_engineered[numeric_features + categorical_features].copy()\n",
    "X_processed = pd.DataFrame(\n",
    "    preprocessor.transform(X_engineered),\n",
    "    columns=preprocessor.get_feature_names()\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… CML preprocessing complete\")\n",
    "print(f\"   CML format (one-hot encoded): {X_processed.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ONNX Model Data Preparation\n",
    "# ============================================================================\n",
    "# The ONNX model expects the ORIGINAL 20 features (no engineering) with underscores\n",
    "\n",
    "onnx_numeric_features = [\n",
    "    'age', 'duration', 'campaign', 'pdays', 'previous',\n",
    "    'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed'\n",
    "]\n",
    "\n",
    "onnx_categorical_features = [\n",
    "    'job', 'marital', 'education', 'default', 'housing', \n",
    "    'loan', 'contact', 'month', 'day_of_week', 'poutcome'\n",
    "]\n",
    "\n",
    "# Rename columns to match ONNX expectations (dots â†’ underscores)\n",
    "df_onnx = df_raw.copy()\n",
    "df_onnx = df_onnx.rename(columns={\n",
    "    'emp.var.rate': 'emp_var_rate',\n",
    "    'cons.price.idx': 'cons_price_idx',\n",
    "    'cons.conf.idx': 'cons_conf_idx',\n",
    "    'nr.employed': 'nr_employed'\n",
    "})\n",
    "\n",
    "# Select only the features the ONNX model expects\n",
    "X_onnx = df_onnx[onnx_numeric_features + onnx_categorical_features].copy()\n",
    "\n",
    "print(f\"\\nâœ… ONNX preprocessing complete\")\n",
    "print(f\"   ONNX format (original features): {X_onnx.shape}\")\n",
    "print(f\"   Features: {len(onnx_numeric_features)} numerical + {len(onnx_categorical_features)} categorical = 20 total\")\n",
    "print(f\"\\nðŸ“Š Ready to test {len(X_processed)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Baseline - CML Model Endpoint\n",
    "\n",
    "## What is CML Models?\n",
    "\n",
    "CML Models is designed for **development and testing**:\n",
    "- âœ… Quick deployment for data science experimentation\n",
    "- âœ… Custom REST API with simple access key auth\n",
    "- âœ… Good for small-to-medium workloads\n",
    "- âš ï¸ Limited scalability and enterprise features\n",
    "\n",
    "## Configuration\n",
    "\n",
    "**Update these values from your Module 1 deployment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model endpoint configuration from shared_utils.config\n",
    "from shared_utils.config import MODEL_ENDPOINT_CONFIG\n",
    "\n",
    "CML_MODEL_ENDPOINT = MODEL_ENDPOINT_CONFIG.get(\"model_endpoint\")\n",
    "CML_ACCESS_KEY = MODEL_ENDPOINT_CONFIG.get(\"access_key\")\n",
    "\n",
    "if not CML_MODEL_ENDPOINT or not CML_ACCESS_KEY:\n",
    "    raise ValueError(\n",
    "        \"Missing model_endpoint or access_key in shared_utils/config.py\\n\"\n",
    "        \"Please update MODEL_ENDPOINT_CONFIG in shared_utils/config.py with your credentials.\"\n",
    "    )\n",
    "\n",
    "print(\"âœ… CML Model endpoint configured\")\n",
    "print(f\"   Endpoint: {CML_MODEL_ENDPOINT}\")\n",
    "print(f\"   Auth: API Access Key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Single Predictions (10 samples)\n",
    "\n",
    "### CML API Format\n",
    "\n",
    "CML Models uses a custom format:\n",
    "```json\n",
    "{\n",
    "  \"accessKey\": \"your_key\",\n",
    "  \"request\": {\n",
    "    \"dataframe_split\": {\n",
    "      \"columns\": [\"feature1\", \"feature2\", ...],\n",
    "      \"data\": [[value1, value2, ...]]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 10 samples for testing\n",
    "test_samples = X_processed.head(10)\n",
    "\n",
    "cml_latencies = []\n",
    "cml_predictions = []\n",
    "\n",
    "print(\"ðŸ”„ Testing CML Model Endpoint (10 single predictions)...\\n\")\n",
    "\n",
    "for idx in range(len(test_samples)):\n",
    "    # Get single row\n",
    "    row = test_samples.iloc[idx]\n",
    "    \n",
    "    # Handle NaN/inf values\n",
    "    row_dict = row.to_dict()\n",
    "    for key, val in row_dict.items():\n",
    "        if pd.isna(val):\n",
    "            row_dict[key] = 0\n",
    "        elif np.isinf(val):\n",
    "            row_dict[key] = 1e10 if val > 0 else -1e10\n",
    "        else:\n",
    "            row_dict[key] = float(val)\n",
    "    \n",
    "    # Create CML API payload\n",
    "    payload = {\n",
    "        \"accessKey\": CML_ACCESS_KEY,\n",
    "        \"request\": {\n",
    "            \"dataframe_split\": {\n",
    "                \"columns\": list(row_dict.keys()),\n",
    "                \"data\": [list(row_dict.values())]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Time the request\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            CML_MODEL_ENDPOINT,\n",
    "            data=json.dumps(payload),\n",
    "            headers={'Content-Type': 'application/json'},\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        latency = (time.time() - start_time) * 1000  # Convert to ms\n",
    "        cml_latencies.append(latency)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            prediction = result['response']['prediction'][0]\n",
    "            cml_predictions.append(prediction)\n",
    "            pred_label = \"YES\" if prediction == 1 else \"NO\"\n",
    "            print(f\"  Sample {idx+1:2d}: {pred_label:3s} | Latency: {latency:7.2f}ms\")\n",
    "        else:\n",
    "            print(f\"  Sample {idx+1:2d}: ERROR - Status {response.status_code}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Sample {idx+1:2d}: ERROR - {str(e)[:50]}\")\n",
    "\n",
    "# Calculate statistics\n",
    "if cml_latencies:\n",
    "    cml_avg_latency = np.mean(cml_latencies)\n",
    "    cml_p50_latency = np.median(cml_latencies)\n",
    "    cml_p95_latency = np.percentile(cml_latencies, 95)\n",
    "    cml_p99_latency = np.percentile(cml_latencies, 99)\n",
    "else:\n",
    "    cml_avg_latency = cml_p50_latency = cml_p95_latency = cml_p99_latency = 0\n",
    "\n",
    "print(f\"\\nðŸ“Š CML Model - Single Prediction Statistics:\")\n",
    "print(f\"   Successful: {len(cml_predictions)}/10\")\n",
    "print(f\"   Avg Latency:  {cml_avg_latency:7.2f}ms\")\n",
    "print(f\"   P50 Latency:  {cml_p50_latency:7.2f}ms\")\n",
    "print(f\"   P95 Latency:  {cml_p95_latency:7.2f}ms\")\n",
    "print(f\"   P99 Latency:  {cml_p99_latency:7.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Batch Prediction (50 samples)\n",
    "\n",
    "Test throughput with a larger batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare batch of 50 samples\n",
    "batch_samples = X_processed.head(50)\n",
    "\n",
    "print(\"ðŸ”„ Testing CML Model Endpoint (50-sample batch)...\\n\")\n",
    "\n",
    "# We'll send them one at a time to measure throughput\n",
    "batch_start = time.time()\n",
    "batch_predictions = []\n",
    "\n",
    "for idx in range(len(batch_samples)):\n",
    "    row = batch_samples.iloc[idx]\n",
    "    \n",
    "    # Handle NaN/inf\n",
    "    row_dict = row.to_dict()\n",
    "    for key, val in row_dict.items():\n",
    "        if pd.isna(val):\n",
    "            row_dict[key] = 0\n",
    "        elif np.isinf(val):\n",
    "            row_dict[key] = 1e10 if val > 0 else -1e10\n",
    "        else:\n",
    "            row_dict[key] = float(val)\n",
    "    \n",
    "    payload = {\n",
    "        \"accessKey\": CML_ACCESS_KEY,\n",
    "        \"request\": {\n",
    "            \"dataframe_split\": {\n",
    "                \"columns\": list(row_dict.keys()),\n",
    "                \"data\": [list(row_dict.values())]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            CML_MODEL_ENDPOINT,\n",
    "            data=json.dumps(payload),\n",
    "            headers={'Content-Type': 'application/json'},\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            batch_predictions.append(result['response']['prediction'][0])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "cml_batch_time = (time.time() - batch_start) * 1000  # ms\n",
    "cml_throughput = len(batch_predictions) / (cml_batch_time / 1000)  # predictions/sec\n",
    "\n",
    "print(f\"âœ… Batch prediction complete\")\n",
    "print(f\"   Successful predictions: {len(batch_predictions)}/50\")\n",
    "print(f\"   Total time: {cml_batch_time:,.2f}ms\")\n",
    "print(f\"   Avg per sample: {cml_batch_time/50:7.2f}ms\")\n",
    "print(f\"   Throughput: {cml_throughput:7.2f} predictions/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Production - AI Inference Service\n",
    "\n",
    "## What is Cloudera AI Inference Service?\n",
    "\n",
    "AI Inference Service is built for **enterprise production**:\n",
    "- ðŸš€ Up to 36x faster inference (GPU) / 4x faster (CPU)\n",
    "- ðŸ”’ JWT token authentication (enterprise security)\n",
    "- ðŸ“Š Built-in autoscaling and high availability\n",
    "- ðŸŒ Industry-standard Open Inference Protocol\n",
    "- âš™ï¸ Powered by NVIDIA Triton Inference Server\n",
    "- ðŸ“ˆ Full enterprise monitoring and observability\n",
    "\n",
    "## Configuration\n",
    "\n",
    "**Update these values from your AI Inference Service deployment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Inference Service Configuration\n",
    "# TODO: Update with your deployment details\n",
    "\n",
    "# JWT token (typically at /tmp/jwt in CML workbench)\n",
    "try:\n",
    "    API_KEY = json.load(open(\"/tmp/jwt\"))[\"access_token\"]\n",
    "    print(\"âœ… JWT token loaded from /tmp/jwt\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸  JWT token not found at /tmp/jwt, using environment variable\")\n",
    "    API_KEY = os.environ.get(\"CDP_TOKEN\", \"\")\n",
    "\n",
    "# Your AI Inference Service endpoint URL and model name\n",
    "#BASE_URL = 'https://ml-XXXXX.cloudera.site/namespaces/serving-default/endpoints/your-model'\n",
    "#MODEL_NAME = 'your-model-id'\n",
    "BASE_URL = 'https://ml-64288d82-5dd.go01-dem.ylcu-atmi.cloudera.site/namespaces/serving-default/endpoints/banking-classifier-ozarate'\n",
    "MODEL_NAME = '6nyo-l4ge-kbb9-odsy'\n",
    "\n",
    "# Setup HTTPX client\n",
    "headers = {\n",
    "    'Authorization': 'Bearer ' + API_KEY,\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "httpx_client = httpx.Client(headers=headers)\n",
    "client = OpenInferenceClient(base_url=BASE_URL, httpx_client=httpx_client)\n",
    "\n",
    "print(\"âœ… AI Inference Service client configured\")\n",
    "print(f\"   Endpoint: {BASE_URL[:60]}...\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Auth: JWT Token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Metadata - Open Inference Protocol Standard\n",
    "\n",
    "One key advantage: standardized model introspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ” Checking server readiness...\\n\")\n",
    "\n",
    "try:\n",
    "    client.check_server_readiness()\n",
    "    print(\"âœ… Server is ready and healthy\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Server check failed: {e}\\n\")\n",
    "\n",
    "# Get model metadata\n",
    "print(\"ðŸ“‹ Retrieving model metadata...\\n\")\n",
    "\n",
    "try:\n",
    "    metadata = client.read_model_metadata(MODEL_NAME)\n",
    "    metadata_dict = json.loads(metadata.json())\n",
    "    \n",
    "    print(f\"Model: {metadata_dict.get('name', 'N/A')}\")\n",
    "    print(f\"Platform: {metadata_dict.get('platform', 'N/A')}\")\n",
    "    \n",
    "    # Show input schema\n",
    "    if 'inputs' in metadata_dict:\n",
    "        print(f\"\\nðŸ“¥ Inputs ({len(metadata_dict['inputs'])} features):\")\n",
    "        for inp in metadata_dict['inputs'][:5]:\n",
    "            print(f\"   â€¢ {inp['name']:20s} | {inp['datatype']:6s} | shape: {inp['shape']}\")\n",
    "        if len(metadata_dict['inputs']) > 5:\n",
    "            print(f\"   ... and {len(metadata_dict['inputs'])-5} more features\")\n",
    "    \n",
    "    # Show output schema  \n",
    "    if 'outputs' in metadata_dict:\n",
    "        print(f\"\\nðŸ“¤ Outputs:\")\n",
    "        for out in metadata_dict['outputs']:\n",
    "            print(f\"   â€¢ {out['name']:20s} | {out['datatype']:6s} | shape: {out['shape']}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to get metadata: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Inference Protocol Data Format\n",
    "\n",
    "The ONNX model expects the **original 20 features** (not engineered features):\n",
    "- âœ… Original feature names with **underscores** (emp_var_rate, not emp.var.rate)\n",
    "- âœ… No feature engineering (no engagement_score, age_group, etc.)\n",
    "- âœ… Raw categorical and numerical values\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"inputs\": [\n",
    "    {\"name\": \"age\", \"shape\": [1,1], \"datatype\": \"FP32\", \"data\": [35.0]},\n",
    "    {\"name\": \"job\", \"shape\": [1,1], \"datatype\": \"BYTES\", \"data\": [\"technician\"]},\n",
    "    {\"name\": \"emp_var_rate\", \"shape\": [1,1], \"datatype\": \"FP32\", \"data\": [1.1]},\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_onnx_inference(row, numeric_feats, categorical_feats):\n",
    "    \"\"\"\n",
    "    Convert pandas row to Open Inference Protocol format for ONNX.\n",
    "    \n",
    "    Args:\n",
    "        row: Single row from DataFrame (pd.Series)\n",
    "        numeric_feats: List of numerical feature names\n",
    "        categorical_feats: List of categorical feature names\n",
    "    \n",
    "    Returns:\n",
    "        List of input dictionaries for InferenceRequest\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    \n",
    "    # Numerical features â†’ FP32\n",
    "    for feat in numeric_feats:\n",
    "        if feat in row.index:\n",
    "            inputs.append({\n",
    "                \"name\": feat,\n",
    "                \"shape\": [1, 1],\n",
    "                \"datatype\": \"FP32\",\n",
    "                \"data\": [float(row[feat])]\n",
    "            })\n",
    "    \n",
    "    # Categorical features â†’ BYTES\n",
    "    for feat in categorical_feats:\n",
    "        if feat in row.index:\n",
    "            inputs.append({\n",
    "                \"name\": feat,\n",
    "                \"shape\": [1, 1],\n",
    "                \"datatype\": \"BYTES\",\n",
    "                \"data\": [str(row[feat])]\n",
    "            })\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "# Test formatter with corrected features\n",
    "print(\"ðŸ§ª Testing data formatter...\\n\")\n",
    "sample_row = X_onnx.iloc[0]\n",
    "formatted = format_for_onnx_inference(sample_row, onnx_numeric_features, onnx_categorical_features)\n",
    "\n",
    "print(f\"âœ… Formatted {len(formatted)} inputs (should be 20)\")\n",
    "print(f\"\\nFirst 5 inputs:\")\n",
    "for inp in formatted[:5]:\n",
    "    print(f\"   {inp['name']:20s} | {inp['datatype']:6s} | {inp['data']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Single Predictions (10 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ONNX-prepared data (original 20 features with correct naming)\n",
    "test_samples_onnx = X_onnx.head(10)\n",
    "\n",
    "ai_latencies = []\n",
    "ai_predictions = []\n",
    "\n",
    "print(\"ðŸ”„ Testing AI Inference Service (10 single predictions)...\\n\")\n",
    "\n",
    "for idx in range(len(test_samples_onnx)):\n",
    "    row = test_samples_onnx.iloc[idx]\n",
    "    \n",
    "    # Format for Open Inference Protocol\n",
    "    inputs = format_for_onnx_inference(row, onnx_numeric_features, onnx_categorical_features)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        pred = client.model_infer(\n",
    "            MODEL_NAME,\n",
    "            request=InferenceRequest(inputs=inputs)\n",
    "        )\n",
    "        \n",
    "        latency = (time.time() - start_time) * 1000  # ms\n",
    "        ai_latencies.append(latency)\n",
    "        \n",
    "        # Parse response\n",
    "        response_dict = json.loads(pred.json())\n",
    "        prediction = response_dict['outputs'][0]['data'][0]\n",
    "        ai_predictions.append(prediction)\n",
    "        \n",
    "        pred_label = \"YES\" if prediction == 1 else \"NO\"\n",
    "        print(f\"  Sample {idx+1:2d}: {pred_label:3s} | Latency: {latency:7.2f}ms\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Sample {idx+1:2d}: ERROR - {str(e)[:50]}\")\n",
    "\n",
    "# Calculate statistics\n",
    "if ai_latencies:\n",
    "    ai_avg_latency = np.mean(ai_latencies)\n",
    "    ai_p50_latency = np.median(ai_latencies)\n",
    "    ai_p95_latency = np.percentile(ai_latencies, 95)\n",
    "    ai_p99_latency = np.percentile(ai_latencies, 99)\n",
    "else:\n",
    "    ai_avg_latency = ai_p50_latency = ai_p95_latency = ai_p99_latency = 0\n",
    "\n",
    "print(f\"\\nðŸ“Š AI Inference Service - Single Prediction Statistics:\")\n",
    "print(f\"   Successful: {len(ai_predictions)}/10\")\n",
    "print(f\"   Avg Latency:  {ai_avg_latency:7.2f}ms\")\n",
    "print(f\"   P50 Latency:  {ai_p50_latency:7.2f}ms\")\n",
    "print(f\"   P95 Latency:  {ai_p95_latency:7.2f}ms\")\n",
    "print(f\"   P99 Latency:  {ai_p99_latency:7.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Batch Prediction (50 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare batch of 50 samples\n",
    "batch_samples_onnx = X_onnx.head(50)\n",
    "\n",
    "print(\"ðŸ”„ Testing AI Inference Service (50-sample batch)...\\n\")\n",
    "\n",
    "batch_start = time.time()\n",
    "batch_predictions_ai = []\n",
    "\n",
    "for idx in range(len(batch_samples_onnx)):\n",
    "    row = batch_samples_onnx.iloc[idx]\n",
    "    inputs = format_for_onnx_inference(row, onnx_numeric_features, onnx_categorical_features)\n",
    "    \n",
    "    try:\n",
    "        pred = client.model_infer(\n",
    "            MODEL_NAME,\n",
    "            request=InferenceRequest(inputs=inputs)\n",
    "        )\n",
    "        response_dict = json.loads(pred.json())\n",
    "        batch_predictions_ai.append(response_dict['outputs'][0]['data'][0])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "ai_batch_time = (time.time() - batch_start) * 1000  # ms\n",
    "ai_throughput = len(batch_predictions_ai) / (ai_batch_time / 1000)  # predictions/sec\n",
    "\n",
    "print(f\"âœ… Batch prediction complete\")\n",
    "print(f\"   Successful predictions: {len(batch_predictions_ai)}/50\")\n",
    "print(f\"   Total time: {ai_batch_time:,.2f}ms\")\n",
    "print(f\"   Avg per sample: {ai_batch_time/50:7.2f}ms\")\n",
    "print(f\"   Throughput: {ai_throughput:7.2f} predictions/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Side-by-Side Comparison\n",
    "\n",
    "## Performance Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvements\n",
    "latency_improvement = ((cml_avg_latency - ai_avg_latency) / cml_avg_latency * 100) if cml_avg_latency > 0 else 0\n",
    "throughput_improvement = ((ai_throughput - cml_throughput) / cml_throughput * 100) if cml_throughput > 0 else 0\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Authentication',\n",
    "        'API Protocol',\n",
    "        'Model Format',\n",
    "        '',\n",
    "        'Avg Latency (10 samples)',\n",
    "        'P95 Latency',\n",
    "        'P99 Latency',\n",
    "        '',\n",
    "        'Batch Time (50 samples)',\n",
    "        'Throughput (pred/sec)',\n",
    "        '',\n",
    "        'Purpose',\n",
    "        'Scale',\n",
    "        'Availability',\n",
    "        'Monitoring'\n",
    "    ],\n",
    "    'CML Models': [\n",
    "        'API Key',\n",
    "        'Custom REST',\n",
    "        'Pickled sklearn',\n",
    "        '',\n",
    "        f'{cml_avg_latency:.2f} ms',\n",
    "        f'{cml_p95_latency:.2f} ms',\n",
    "        f'{cml_p99_latency:.2f} ms',\n",
    "        '',\n",
    "        f'{cml_batch_time:,.0f} ms',\n",
    "        f'{cml_throughput:.2f}',\n",
    "        '',\n",
    "        'Development/Testing',\n",
    "        'Small-Medium',\n",
    "        'Basic',\n",
    "        'Basic metrics'\n",
    "    ],\n",
    "    'AI Inference Service': [\n",
    "        'JWT Token',\n",
    "        'Open Inference Protocol',\n",
    "        'ONNX',\n",
    "        '',\n",
    "        f'{ai_avg_latency:.2f} ms',\n",
    "        f'{ai_p95_latency:.2f} ms',\n",
    "        f'{ai_p99_latency:.2f} ms',\n",
    "        '',\n",
    "        f'{ai_batch_time:,.0f} ms',\n",
    "        f'{ai_throughput:.2f}',\n",
    "        '',\n",
    "        'Production Serving',\n",
    "        'Enterprise Scale',\n",
    "        'HA + Autoscaling',\n",
    "        'Full observability'\n",
    "    ],\n",
    "    'Improvement': [\n",
    "        'Enterprise security',\n",
    "        'Industry standard',\n",
    "        'Optimized format',\n",
    "        '',\n",
    "        f'{latency_improvement:+.1f}%' if latency_improvement != 0 else '-',\n",
    "        f'{((cml_p95_latency - ai_p95_latency) / cml_p95_latency * 100):+.1f}%' if cml_p95_latency > 0 else '-',\n",
    "        f'{((cml_p99_latency - ai_p99_latency) / cml_p99_latency * 100):+.1f}%' if cml_p99_latency > 0 else '-',\n",
    "        '',\n",
    "        f'{((cml_batch_time - ai_batch_time) / cml_batch_time * 100):+.1f}%' if cml_batch_time > 0 else '-',\n",
    "        f'{throughput_improvement:+.1f}%' if throughput_improvement != 0 else '-',\n",
    "        '',\n",
    "        'â†’ Production ready',\n",
    "        'â†’ Handles more load',\n",
    "        'â†’ Always available',\n",
    "        'â†’ Full visibility'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ðŸ“Š DEPLOYMENT COMPARISON: Development â†’ Production\")\n",
    "print(\"=\"*100)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways for Customer Conversations\n",
    "\n",
    "### 1. **Performance Story**\n",
    "- AI Inference Service delivers measurable latency improvements\n",
    "- ONNX optimization + Triton runtime = faster inference\n",
    "- Concrete numbers to show customers: \"X% faster response times\"\n",
    "\n",
    "### 2. **Enterprise Readiness**\n",
    "- **Security**: API keys â†’ JWT tokens (enterprise authentication)\n",
    "- **Standards**: Custom API â†’ Open Inference Protocol (no vendor lock-in)\n",
    "- **Operations**: Basic monitoring â†’ Full observability stack\n",
    "\n",
    "### 3. **Scale & Availability**\n",
    "- **CML Models**: Good for dev/test, limited production scale\n",
    "- **AI Inference Service**: Built for production with autoscaling and HA\n",
    "- **Cost efficiency**: Scale-to-zero when idle, scale-up under load\n",
    "\n",
    "### 4. **The Evolution Path**\n",
    "This is the natural progression:\n",
    "1. Start with CML Models for development\n",
    "2. Monitor performance and detect issues\n",
    "3. Optimize and convert to ONNX\n",
    "4. Deploy to AI Inference Service for production\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Production Monitoring & Operations\n",
    "\n",
    "## Where to Find Operational Visibility\n",
    "\n",
    "AI Inference Service provides enterprise-grade monitoring:\n",
    "\n",
    "### 1. **Model Endpoint Dashboard**\n",
    "- Navigate to: **CDP Console â†’ Machine Learning â†’ AI Inference Service â†’ Your Endpoint**\n",
    "- View:\n",
    "  - Request rate and latency graphs\n",
    "  - Replica count (autoscaling status)\n",
    "  - Error rates and health checks\n",
    "  - Resource utilization (CPU/GPU/Memory)\n",
    "\n",
    "### 2. **Grafana Dashboards**\n",
    "- Access: **Cloudera AI Workbenches â†’ Actions â†’ Open Grafana**\n",
    "- Pre-built dashboards for:\n",
    "  - Inference latency percentiles (P50, P95, P99)\n",
    "  - Throughput over time\n",
    "  - Model-specific metrics\n",
    "  - Infrastructure health\n",
    "\n",
    "### 3. **Logs and Debugging**\n",
    "- Model deployment logs show startup and errors\n",
    "- Triton server logs for detailed inference traces\n",
    "- Integration with enterprise log aggregation systems\n",
    "\n",
    "### 4. **Alerting**\n",
    "- Configure alerts on latency thresholds\n",
    "- Error rate spikes\n",
    "- Resource exhaustion warnings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop Complete! ðŸŽ‰\n",
    "\n",
    "### You've Experienced the Full MLOps Lifecycle:\n",
    "\n",
    "âœ… **Module 1**: Model training and development deployment  \n",
    "âœ… **Module 2**: Monitoring and drift detection  \n",
    "âœ… **Module 3**: Automated retraining and ONNX conversion  \n",
    "âœ… **Module 4**: Production deployment with enterprise features  \n",
    "\n",
    "### What to Tell Customers:\n",
    "\n",
    "1. **\"Cloudera AI covers the complete ML lifecycle\"** - From experimentation to production\n",
    "2. **\"Built-in optimization\"** - ONNX conversion and Triton runtime for performance\n",
    "3. **\"Enterprise-grade from day one\"** - Security, monitoring, and scale built in\n",
    "4. **\"No vendor lock-in\"** - Open standards (Open Inference Protocol, ONNX)\n",
    "5. **\"Proven path to production\"** - Clear evolution from dev to production\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Explore the monitoring dashboards\n",
    "- Test autoscaling behavior under load\n",
    "- Try deploying your own models\n",
    "- Build customer demos with your use cases\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for completing the workshop!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
