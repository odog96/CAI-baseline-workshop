{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Model Metrics Analysis\n",
    "\n",
    "## Hands-On Lab: Analyzing Model Performance Over Time\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this notebook, you'll analyze **all model metrics collected** during the monitoring pipeline execution. You'll examine:\n",
    "\n",
    "- **Accuracy trends** over multiple time periods\n",
    "- **Precision, Recall, and F1 scores** across periods\n",
    "- **Degradation patterns** and detection\n",
    "- **Expected vs. actual accuracy** comparison\n",
    "- **Statistical analysis** of model performance\n",
    "\n",
    "This is the **final piece** of the monitoring workflow, where you synthesize insights from:\n",
    "- Script 02: `02_prepare_artificial_data.py` (creates ground truth with degradation)\n",
    "- Script 03: `03_monitoring_pipeline.py` (collects metrics across periods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "By the end of this notebook, you'll:\n",
    "\n",
    "1. ✅ **Load and parse** monitoring results from JSON files\n",
    "2. ✅ **Visualize** accuracy degradation patterns\n",
    "3. ✅ **Identify** when and where model degradation is detected\n",
    "4. ✅ **Compare** actual vs. expected accuracy by period\n",
    "5. ✅ **Analyze** precision-recall tradeoffs\n",
    "6. ✅ **Understand** the complete monitoring pipeline flow\n",
    "7. ✅ **Interpret** degradation thresholds and their impact\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Data Loading\n",
    "\n",
    "### Step 1.1: Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print('✓ Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Load Monitoring Results\n",
    "\n",
    "The `monitoring_results.json` file contains comprehensive metrics collected from the monitoring pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "DATA_DIR = Path('../data')  # Relative to module2/\n",
    "\n",
    "monitoring_results_path = DATA_DIR / 'monitoring_results.json'\n",
    "metadata_path = DATA_DIR / 'ground_truth_metadata.json'\n",
    "\n",
    "# Check if files exist\n",
    "if not monitoring_results_path.exists():\n",
    "    print(f'❌ Error: {monitoring_results_path} not found')\n",
    "    print(f'   Make sure you have run the monitoring pipeline first')\n",
    "else:\n",
    "    print(f'✓ Found monitoring results at: {monitoring_results_path}')\n",
    "\n",
    "if not metadata_path.exists():\n",
    "    print(f'❌ Error: {metadata_path} not found')\n",
    "else:\n",
    "    print(f'✓ Found metadata at: {metadata_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load monitoring results\n",
    "with open(monitoring_results_path, 'r') as f:\n",
    "    monitoring_results = json.load(f)\n",
    "\n",
    "# Load metadata\n",
    "with open(metadata_path, 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('MONITORING PIPELINE SUMMARY')\n",
    "print('='*80)\n",
    "print(f'\\nStatus: {monitoring_results[\"status\"].upper()}')\n",
    "print(f'Periods Processed: {monitoring_results[\"num_periods_processed\"]}')\n",
    "print(f'\\nMetadata:')\n",
    "print(f'  Total Samples: {metadata[\"total_samples\"]}')\n",
    "print(f'  Total Periods: {metadata[\"num_periods\"]}')\n",
    "print(f'  Samples per Period: {metadata[\"samples_per_period\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Overview of Collected Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract period data into a DataFrame\n",
    "periods_data = []\n",
    "\n",
    "for period_result in monitoring_results['periods']:\n",
    "    period_dict = {\n",
    "        'period': period_result['period'],\n",
    "        'num_samples': period_result['num_samples'],\n",
    "        'accuracy': period_result['metrics']['accuracy'],\n",
    "        'precision': period_result['metrics']['precision'],\n",
    "        'recall': period_result['metrics']['recall'],\n",
    "        'f1': period_result['metrics']['f1'],\n",
    "        'previous_accuracy': period_result['previous_accuracy'],\n",
    "        'accuracy_drop': period_result['accuracy_drop'],\n",
    "        'is_degraded': period_result['is_degraded']\n",
    "    }\n",
    "    periods_data.append(period_dict)\n",
    "\n",
    "df_periods = pd.DataFrame(periods_data)\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('PERIOD METRICS SUMMARY')\n",
    "print('='*80)\n",
    "print()\n",
    "print(df_periods.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Accuracy Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize accuracy trends\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Model Performance Analysis', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Accuracy trend\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(df_periods['period'], df_periods['accuracy']*100, \n",
    "         marker='o', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Period')\n",
    "ax1.set_ylabel('Accuracy (%)')\n",
    "ax1.set_title('Accuracy Over Time')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics comparison\n",
    "ax2 = axes[0, 1]\n",
    "x = np.arange(len(df_periods))\n",
    "width = 0.2\n",
    "ax2.bar(x - 1.5*width, df_periods['accuracy']*100, width, label='Accuracy')\n",
    "ax2.bar(x - 0.5*width, df_periods['precision']*100, width, label='Precision')\n",
    "ax2.bar(x + 0.5*width, df_periods['recall']*100, width, label='Recall')\n",
    "ax2.bar(x + 1.5*width, df_periods['f1']*100, width, label='F1')\n",
    "ax2.set_xlabel('Period')\n",
    "ax2.set_ylabel('Score (%)')\n",
    "ax2.set_title('All Metrics by Period')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(df_periods['period'].astype(int))\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Accuracy drops\n",
    "ax3 = axes[1, 0]\n",
    "drops = df_periods['accuracy_drop'].fillna(0)*100\n",
    "ax3.bar(df_periods['period'], drops)\n",
    "ax3.axhline(y=monitoring_results['configuration']['degradation_threshold']*100, \n",
    "             color='red', linestyle='--', linewidth=2, label='Threshold')\n",
    "ax3.set_xlabel('Period')\n",
    "ax3.set_ylabel('Drop (%)')\n",
    "ax3.set_title('Accuracy Drop')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Precision vs Recall\n",
    "ax4 = axes[1, 1]\n",
    "ax4.scatter(df_periods['recall']*100, df_periods['precision']*100, s=200)\n",
    "for _, row in df_periods.iterrows():\n",
    "    ax4.annotate(f\"P{int(row['period'])}\", \n",
    "                xy=(row['recall']*100, row['precision']*100),\n",
    "                fontsize=9, ha='center', va='center')\n",
    "ax4.set_xlabel('Recall (%)')\n",
    "ax4.set_ylabel('Precision (%)')\n",
    "ax4.set_title('Precision vs Recall')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nVisualizations created successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Detailed Metrics Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*80)\n",
    "print('DETAILED PERIOD ANALYSIS')\n",
    "print('='*80)\n",
    "\n",
    "config = monitoring_results['configuration']\n",
    "\n",
    "for _, row in df_periods.iterrows():\n",
    "    period = int(row['period'])\n",
    "    print(f'\\nPeriod {period}:')\n",
    "    print(f'  Samples: {row[\"num_samples\"]}')\n",
    "    print(f'  Accuracy: {row[\"accuracy\"]*100:.2f}%')\n",
    "    print(f'  Precision: {row[\"precision\"]*100:.2f}%')\n",
    "    print(f'  Recall: {row[\"recall\"]*100:.2f}%')\n",
    "    print(f'  F1 Score: {row[\"f1\"]*100:.2f}%')\n",
    "    \n",
    "    if pd.notna(row['accuracy_drop']):\n",
    "        print(f'  Accuracy Drop: {row[\"accuracy_drop\"]*100:.2f}%')\n",
    "        print(f'  Degraded: {\"Yes\" if row[\"is_degraded\"] else \"No\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Summary Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*80)\n",
    "print('SUMMARY STATISTICS')\n",
    "print('='*80)\n",
    "\n",
    "print(f'\\nAccuracy:')\n",
    "print(f'  Mean: {df_periods[\"accuracy\"].mean()*100:.2f}%')\n",
    "print(f'  Min: {df_periods[\"accuracy\"].min()*100:.2f}%')\n",
    "print(f'  Max: {df_periods[\"accuracy\"].max()*100:.2f}%')\n",
    "\n",
    "print(f'\\nPrecision:')\n",
    "print(f'  Mean: {df_periods[\"precision\"].mean()*100:.2f}%')\n",
    "\n",
    "print(f'\\nRecall:')\n",
    "print(f'  Mean: {df_periods[\"recall\"].mean()*100:.2f}%')\n",
    "\n",
    "print(f'\\nF1 Score:')\n",
    "print(f'  Mean: {df_periods[\"f1\"].mean()*100:.2f}%')\n",
    "\n",
    "num_degraded = df_periods['is_degraded'].sum()\n",
    "print(f'\\nDegradation Events: {num_degraded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Conclusions\n",
    "\n",
    "You have successfully completed the Model Metrics Analysis notebook!\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Model monitoring requires continuous tracking** of performance metrics across time\n",
    "2. **Multiple metrics matter**: Accuracy alone doesn't tell the complete story\n",
    "3. **Thresholds are critical**: Both absolute minimum and degradation rate detection\n",
    "4. **Automation enables scale**: The CML job framework runs the entire monitoring pipeline automatically\n",
    "5. **Degradation detection is actionable**: Systems can respond automatically to model drift\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Explore different threshold values for your use case\n",
    "- Consider additional metrics (AUC, ROC curves, confusion matrices)\n",
    "- Implement automated retraining when degradation is detected\n",
    "- Extend monitoring to production model deployments\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
